\chapter{Experimental Results}
\label{chp:results}
As explained in \chapref{chp:techbackground}, the flock pattern detection relies mainly in 3 parameters, namely
\textit{number of trajectories} ({$\mu$), \textit{flock extension (or length)} ($\delta$) and \textit{disk radius}
($\epsilon/2$). Those parameters impact significantly in the number of patterns found as well as the time taken to find
those patterns, depending on the dataset being analyzed. Before evaluating our proposed algorithm in the target
datasets, we first gathered some metadata information about them, in order to choose wisely the value for the
aforementioned parameters, so we can indeed find a good number of flocks. We collected that metadata by running the
algorithm multiple times with values for the parameters based on the dataset description and them increased or
decreased the values according the number of flocks that we were able to find. After finding at least 100 flock
patterns, we then setle on a range of values for those parameters and used them to evaluate each dataset.

We implemented the system architecture proposed in \secref{sec:architecture} in C++, using g++ 4.8.4 and the C++11
\citep{cpp11spec} features. With the system architecture in place, we then implemented the GSB data listener and the FP
data processor.

Our comparison experiments were performed in a linux box with Intel Xeon Quad processor and 14GB of main memory running
Ubuntu Server 14.04. We used four datasets (real and synthetic) in our experiments, with some of them having more than
50M entries and 2K unique $O_{id}$.

We compare BitDF running time and number of disks generated against an implementation of BFE, running in the same
machine, using the same datasets and parameter values. The metrics used for each dataset were (a) CPU time and (b)
Number of disks. In (a), for each flock parameter ($\mu$, $\delta$ and $\epsilon$) we fixed it in a specific value and
varied the others, in order to see how the algorithm behaves. For (b), we analyzed the cumulative number of disks
generated over time.

Before showing the results, there are some analysis outcomes that will hold for any dataset analyzed here:

\begin{enumerate}
    \item \textbf{$\delta$ variation}: The longer the flock patterns we try to find (long $\delta$), the more disks will
        stay cached being analyzed and trying to be merged with new disks from time slots to come. This can have a big
        impact in running time.\label{sssec:lvariation}

    \item \textbf{$\epsilon$ variation}: As the disk radius ($\epsilon$) gets bigger, more points will be clustered
        inside a disk and thus more intersections and duplicates of those disks as more likely to be founf. This will
        affect the time spent in analyzing disks from one time slot to another. \label{sssec:gvariation}

    \item \textbf{$\mu$ variation}: By increasing $\mu$, it gets more and more difficult to find disks that are flock
        candidates ($|D| \ge \mu$), so less disks are generated. This scenario is where BitDF will achieve less
        improvements. \label{sssec:nvariation}
\end{enumerate}

\section{Trucks Dataset}
\label{sec:trucks}
This was one of the datasets that Vieira et al. \citep{vieira} used in the experiments of BFE, but the authors modified
such dataset \citep{trucksdataset}, resulting in a data set which is way far from those found in real world analysis. In
their modification, every time interval is of one second, the GPS coordinates were mapped to a $\mathbb{R}^2$ coordinate
system (ranging from 0 to 1000) and most of the points are present in each time interval. The modified dataset resulted
in 112203 entries and 276 unique $O_{id}$ (instead of 50 in the original dataset).

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/Trucks_n_4_g_1_5_varying_l.eps}
        \caption{$\mu = 4$, $\epsilon = 1.5$ and $\delta$ varying}
        \label{fig:trucks_vary_l}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/Trucks_n_4_l_20_varying_g.eps}
        \caption{$\mu = 4$, $\delta = 20$ and $\epsilon$ varying}
        \label{fig:trucks_vary_g}
    \end{subfigure}
    \caption{Results varying $\delta$ and $\epsilon$ for Trucks dataset}
    \label{fig:trucks_results}
\end{figure*}

\begin{figure*}[h!]
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/Trucks_l_20_g_1_5_varying_n.eps}
        \caption{$\delta = 20$, $\epsilon = 1.5$ and $\mu$ varying}
        \label{fig:trucks_vary_n}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/Trucks_d.eps}
        \caption{Cumulative disks by time}
        \label{fig:trucks_disks}
    \end{subfigure}
    \caption{Results varying $\mu$ and number of disks generated over time for Trucks dataset}
    \label{fig:trucks_results2}
\end{figure*}

By looking at \figref{fig:trucks_results} and \figref{fig:trucks_results2}, we can see that we have some gains in
execution time. However, they are not too significant due to the fact that the number of disks generated by each time
slot does not differ too much between BFE and BitDF, as we can see in \figref{fig:trucks_disks}. This happens because
almost all points appear in every single time slot, then buffering and mapping the $O_{id}$ presence in time does not
make a big impact, since we will not be able to filter out disks created with points not being present in $\delta$
consecutive time slots.  \figref{fig:trucks_vary_l} and \figref{fig:trucks_vary_g} show some running time improvements
against BFE, which are backed up by the explanations given at~\ref{sssec:lvariation} and~\ref{sssec:gvariation}. A
different behavior is observed in \figref{fig:trucks_vary_n}, in which BitDF starts better but ends up almost tied with
BFE, which can be explained by~\ref{sssec:nvariation}, but is also very influenced by the dataset modifications.

\section{BerlinMOD Dataset}
\label{sec:berlinmod}
BerlinMOD consists in a traffic generation model \citep{berlinmodpaper} used to create sythentic datasets of moving
objects. This particular dataset that we are analysing was the biggest one that we could find in the set of synthetic
datasets that are available in their website \citep{berlinmod} and consists of 56,127,943 entries and 2000 unique
$O_{id}$.

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/BerlinMOD_n_4_g_100_varying_l.eps}
        \caption{$\mu = 4$, $\epsilon = 100$ and $\delta$ varying}
        \label{fig:berlinmod_vary_l}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/BerlinMOD_n_4_l_8_varying_g.eps}
        \caption{$\mu = 4$, $\delta = 8$ and $\epsilon$ varying}
        \label{fig:berlinmod_vary_g}
    \end{subfigure}
    \caption{Results varying $\delta$ and $\epsilon$ for BerlinMOD dataset}
    \label{fig:berlinmod_results}
\end{figure*}

\begin{figure*}[h!]
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/BerlinMOD_l_8_g_100_varying_n.eps}
        \caption{$\delta = 8$, $\epsilon = 100$ and $\mu$ varying}
        \label{fig:berlinmod_vary_n}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/BerlinMOD_d.eps}
        \caption{Cumulative disks by time}
        \label{fig:berlinmod_disks}
    \end{subfigure}
    \caption{Results varying $\mu$ and number of disks generated over time for BerlinMOD dataset}
    \label{fig:berlinmod_results2}
\end{figure*}

As we can see by the results presented in \figref{fig:berlinmod_results} and \figref{fig:berlinmod_results2}, BitDF was
able to achieve great performance gains over BFE, in some cases being 57\% faster (\figref{fig:berlinmod_vary_l}). In
\figref{fig:berlinmod_disks} it is shown that BitDF reduces the cumulative number of disks created over time in 94\%, by
only creating disks that can indeed form flock patterns, which justifies the great improvements that BitDF was able to
get in this dataset. Differently from the results presented in \secref{sec:trucks}, even with a growing $\mu$, BitDF was
able to get great improvements over BFE, ranging from 20\% to 48\%, as depicted in \figref{fig:berlinmod_vary_n}. It is
also worth mentioning the gain in running time that BitDF was able to achieve when we varied the $\epsilon$ parameter,
reaching 48\%, as shown in \figref{fig:berlinmod_vary_g}

\section{TDrive Dataset}
\label{sec:tdrive}
This is a real dataset, having spatio-temporal data describing one week of taxis' trajectories of taxis in Beijing,
China, available in \citep{tdrive}. It has 17,762,489 entries with 10336 unique $O_{id}$.

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/TDrive_n_4_g_100_varying_l.eps}
        \caption{$\mu = 4$, $\epsilon = 100$ and $\delta$ varying}
        \label{fig:tdrive_vary_l}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/TDrive_n_4_l_8_varying_g.eps}
        \caption{$\mu = 4$, $\delta = 8$ and $\epsilon$ varying}
        \label{fig:tdrive_vary_g}
    \end{subfigure}
    \caption{Results varying $\delta$ and $\epsilon$ for TDrive dataset}
    \label{fig:tdrive_results}
\end{figure*}

One can see by the results presented in \figref{fig:tdrive_results} and \figref{fig:tdrive_results2}, that BitDF was
able to dramatically reduce the execution time, when compared to BFE. When varying the $\delta$ parameter, the running
time improvement was of almost 90\%, droping from 1,2265 seconds to only 125 seconds of processing time, as shown in
\figref{fig:tdrive_vary_l}. Continuing the improvements, in \figref{fig:tdrive_vary_g} we can see that BitDF reduced the
execution time up to 74\% when varying $\epsilon$. Additionally, despite seeing some similar behavior with the other
analyzed datasets (like presented in \secref{sec:trucks}) when varying $\mu$, \figref{fig:tdrive_vary_n} shows that
BitDF was able to improve the execution time by 74\% in some cases. It is also worth noting that the results achieved
are a reflex of the huge decrease of disks that were generated by time, as depicted in \figref{fig:tdrive_disks},
reaching almost 96\% of reduction.

\begin{figure*}[h!]
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/TDrive_l_8_g_100_varying_n.eps}
        \caption{$\delta = 8$, $\epsilon = 100$ and $\mu$ varying}
        \label{fig:tdrive_vary_n}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/TDrive_d.eps}
        \caption{Cumulative disks by time}
        \label{fig:tdrive_disks}
    \end{subfigure}
    \caption{Results varying $\mu$ and number of disks generated over time for TDrive dataset}
    \label{fig:tdrive_results2}
\end{figure*}

\section{Brinkkhoff Dataset}
\label{sec:brinkhoff}
Likewise BerlinMOD, Brinkhoff is also a city traffic generation model \citep{brinkhoffpaper}. We generated a synthetic
dataset using the Minnesota Web-based Traffic Generator \citep{mntg}, having 2000 as the "Starting Vehicles" and 100 as
the "Simulation Time" parameters, which are the largest allowed numbers for them. The result dataset has 314,523 entries
and 7000 unique $O_{id}$.

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/Brinkhoff_n_4_g_200_varying_l.eps}
        \caption{$\mu = 4$, $\epsilon = 200$ and $\delta$ varying}
        \label{fig:brinkhoff_vary_l}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/Brinkhoff_n_4_l_8_varying_g.eps}
        \caption{$\mu = 4$, $\delta = 8$ and $\epsilon$ varying}
        \label{fig:brinkhoff_vary_g}
    \end{subfigure}
    \caption{Results varying $\delta$ and $\epsilon$ for Brinkhoff dataset}
    \label{fig:brinkhoff_results}
\end{figure*}

\begin{figure*}[h!]
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/Brinkhoff_l_8_g_200_varying_n.eps}
        \caption{$\delta = 8$, $\epsilon = 200$ and $\mu$ varying}
        \label{fig:brinkhoff_vary_n}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/Brinkhoff_d.eps}
        \caption{Cumulative disks by time}
        \label{fig:brinkhoff_disks}
    \end{subfigure}
    \caption{Results varying $\mu$ and number of disks generated over time for Brinkhoff dataset}
    \label{fig:brinkhoff_results2}
\end{figure*}

The results achieved with this dataset were the best that BitDF was able to get amongst the other datasets analyzed in
this thesis, as one can notice by looking to \figref{fig:brinkhoff_results} and \figref{fig:brinkhoff_results2}. There
were huge drops in execution time, as depicted in \figref{fig:brinkhoff_vary_l}, where BitDF analyzed the whole dataset
in only 69 seconds, while BFE took 12,732 seconds, representing an improvement of 99.5\%. Another great running time
improvement of 99\% can be seen when we varied the $\epsilon$ parameter, as shown in \figref{fig:brinkhoff_vary_g},
droping from 13,141 seconds to 125 seconds. We can see in \figref{fig:brinkhoff_vary_n} that even with the variation of
$\mu$, BitDF was able to show huge improvement, with results ranging from 98\% to 99\% of CPU time reduction.
Additionally, we can see in \figref{fig:brinkhoff_disks} that we were able to reduce the number of disks by 95\%, what
is a huge improvement and is reflecting directly in the running time improvements that we could see with this dataset.

% Multi-thread evaluations
\section{Multi-threaded evaluation}
After implementing the architecture proposed in \secref{sec:architecture}, evaluating it in \chapref{chp:results} and
seeing great improvements in the running time when compared with other algorithms, we decided to test how our system
would perform by taking advantage of the multi-core paradigm that is been widely used nowadays. We then took a step
further and implement a new Data Processor, that we called Parallel Flock Processor (PFP). PFP was implemented having in
mind the multi-threaded model described in \secref{subsec:multithread}, which we call BitDF Multi-treaded (BitDF MT).

In order to see how BitDF MT would perform, we took the worst performances of BitDF for each dataset, from
\chapref{chp:results}, and compared it with BitDF MT running with different number of worker threads executing in
parallel. It is important to notice that when we say that we are running with 4 worker threads, we are actually running
with 8 threads: 4 $c_t$ threads processing different EGCs plus one $d_t$ thread (attached to its $c_t$ thread)
processing the disks generated by its parent $c_i$.

Our benchmarks were executed in a different machine from that one mentioned in the beginning of \chapref{chp:results},
in a way that we opted to get a better multi-core processor setup. With that in mind, we ran our experiments in a Linux
box, running Ubuntu 16.04 LTS, having an Intel Xeon CPU, with 2.3 GHz and 4 physical cores, using Intel Hyper-Threading
technology \citep{hyper} meaning that we would theoretically have 8 different processing units.

Below we will present the benchmarks that were run for each dataset already seen in \chapref{chp:results}, namely
Trucks, TDrive, BerlinMOD and Brinkhoff. For each of the aforementioned datasets, we ran BitDF MT with the same
parameters as the longest BitDF run presented in \chapref{chp:results}, varying the threads from 1 (pure BitDF) to 30.
With variations of 1 worker thread from 1 to 10, 2 worker threads from 10 to 20 and 5 worker threads from 20 to 30. We
set the result of BitDF (1 thread) as being 100\% of the total executing time and highlighted with red color and all
BitDF MT runs are in blue, always being a percentage of the highlighted result. It is worth mentioning that here we are
only evaluating the running time of BitDF MT, because the number of generated disks will be the same as we have seen in
the previous results shown for BitDF in \chapref{chp:results}, since we are still using BitDF as the base flock
processor. Additionally, in order to have a better idea on how BitDF MT outperforms BitDF and BFE, we show some graphs
depicting the running time of them together, for each dataset. For our BitDF MT, we picked 5 and 7 as the number of
worker threads, since those are the values that show the best peformance overall.

First we present the results for the Trucks dataset, which we ran with the following parameters (based on previous
results from \secref{sec:trucks}): $\mu=4$, $\delta=20$ and $\epsilon=1.5$. That dataset would be the most difficult one
to show running time improvements due to its small size and running times being already fast (with the longest one being
around 200 seconds for BitDF). Despite that, we can see in \figref{fig:trucks_threads} that we were able to reduce as
much as 51\% when running with 5 $c_t$ threads, which totalize 10 threads. After that we can see that we could stay
almost stable, not gaining any performance but also not deteriorating it too much. Having it becoming stable is an
expected result, as the processor would start to schedule and pause threads execution, because of having all its
resources busy, as the number of executing threads exceeds too much the number of available processing units.

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/Trucks_complete_varying_l.eps}
        \caption{$\mu = 4$, $\epsilon = 1.5$ and $\delta$ varying}
        \label{fig:trucks_complete_vary_l}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/Trucks_complete_varying_g.eps}
        \caption{$\mu = 4$, $\delta = 20$ and $\epsilon$ varying}
        \label{fig:trucks_complete_vary_g}
    \end{subfigure}
    \caption{Results varying $\delta$ and $\epsilon$ for Trucks dataset}
    \label{fig:trucks_complete_results}
\end{figure*}

\figref{fig:trucks_complete_results} and \figref{fig:trucks_complete_vary_n} show that we were able to achieve more
meaningful results with BitDF MT, than those presented in \secref{sec:trucks} and that the improvements obtained with
BitDF MT runing with both 5 and 7 worker threads were almost the same. We could reduce the running time of BitDF by 18\%
when varying $\mu$ (\figref{fig:trucks_complete_vary_n}) and $\epsilon$ (\figref{fig:trucks_complete_vary_g}) and by
30\% when varying $\delta$ (\figref{fig:trucks_complete_vary_l}).

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{images/Trucks_complete_varying_n.eps}
    \caption{$\delta = 20$, $\epsilon = 1.5$ and $\mu$ varying}
    \label{fig:trucks_complete_vary_n}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{images/Trucks_thread.eps}
    \caption{Execution time reduction by number of threads for the Trucks dataset}
    \label{fig:trucks_threads}
\end{figure}

In \figref{fig:berlinmod_threads}, we show the graph with the results for the BerlinMOD dataset. Due to its large size
and somewhat long execution times (as previously presented in \secref{sec:berlinmod}) we expected to obtain good results
in running BitDF in a multi-threaded fashion. We have set our parameters to have the following values: $\mu=4$,
$\delta=8$ and $\epsilon=200$, as we had those resulting in the longest running time (around 1000 seconds). It is
noticed by \figref{fig:berlinmod_threads} that the results also tend to stabilize when we reach 5 worker threads (10 in
total), but we reach our best running time with 8 worker threads, with an improvement of 62\%. It is also seen that the
running time starts to getting worse as we exceed the number of processing units too much (shown when BitDF is executing
with 30 worker threads, being 60 in total).

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/BerlinMOD_complete_varying_l.eps}
        \caption{$\mu = 4$, $\epsilon = 100$ and $\delta$ varying}
        \label{fig:berlinmod_complete_vary_l}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/BerlinMOD_complete_varying_g.eps}
        \caption{$\mu = 4$, $\delta = 8$ and $\epsilon$ varying}
        \label{fig:berlinmod_complete_vary_g}
    \end{subfigure}
    \caption{Results varying $\delta$ and $\epsilon$ for BerlinMOD dataset}
    \label{fig:berlinmod_complete_results}
\end{figure*}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{images/BerlinMOD_complete_varying_n.eps}
    \caption{$\delta = 8$, $\epsilon = 100$ and $\mu$ varying}
    \label{fig:berlinmod_complete_vary_n}
\end{figure}

By looking at \figref{fig:berlinmod_complete_results} and \figref{fig:berlinmod_complete_vary_n}, one can see that
where BitDF had the worst results is where BitDF MT was able to show how a good multi-threaded remodeling is important
in order to get the most out of multi-core architecture. BitDF MT improved the running time of BitDF by 50\% when
running with both 5 and 7 worker threads, as shown in \figref{fig:berlinmod_complete_vary_l}. BitDF MT also achieved
good results when we varied the $\mu$ parameter, having 45\% of running time decrease as it is depicted in
\figref{fig:berlinmod_complete_vary_n}. Last, but definitely not least, we can se in
\figref{fig:berlinmod_complete_vary_l} that BitDF MT improved the running time of BitDF by 70\%, with both 5 and 7
worker threads.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{images/BerlinMOD_thread.eps}
    \caption{Execution time reduction by number of threads for the BerlinMOD dataset}
    \label{fig:berlinmod_threads}
\end{figure}

TDrive is also a large dataset, but not as large as BerlinMOD and its executing time was not as long as the latter.
Based on the results presented in \secref{sec:tdrive}, we chose the parameter that led to the longest executing time
(around 600 seconds): $\mu=4$, $\delta=4$ and $\epsilon=100$. \figref{fig:tdrive_threads} shows tht the results obey the
same pattern that we have been seeing in the previous results: great improvements in the beginning, but stabilizing as
we exceed the number of processing units in the machine. We can also see some slight improvements even when we evaluate
with more than 5 worker threads. We can see in \figref{fig:tdrive_threads} that we were able to reduce the running time
as much as 70\%, when compared to the single threaded model, which is a huge improvement added to those already achieved
in \secref{sec:tdrive}

When it comes to comparing the running time of BitDF MT against BitDF and BFE, we can see that BitDF MT was able to
improve the running time even more than we have seen in \secref{sec:tdrive}. When varying the parameter $\mu$ we could
reduce the execution time by 60\%, as one can see in \figref{fig:tdrive_complete_vary_n}.
\figref{fig:tdrive_complete_vary_g} also shows improvements of 60\%, when compared to BitDF, when varying the parameter
$\epsilon$. The best result though is when we varied the parameter $\delta$, in which we could improve the BitDF running
time by 70\% when executing BitDF MT with both 5 and 7 worker threads, as depicted in
\figref{fig:tdrive_complete_vary_g}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{images/TDrive_thread.eps}
    \caption{Execution time reduction by number of threads for the TDrive dataset}
    \label{fig:tdrive_threads}
\end{figure}

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/TDrive_complete_varying_l.eps}
        \caption{$\mu = 4$, $\epsilon = 100$ and $\delta$ varying}
        \label{fig:tdrive_complete_vary_l}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/TDrive_complete_varying_g.eps}
        \caption{$\mu = 4$, $\delta = 8$ and $\epsilon$ varying}
        \label{fig:tdrive_complete_vary_g}
    \end{subfigure}
    \caption{Results varying $\delta$ and $\epsilon$ for TDrive dataset}
    \label{fig:tdrive_complete_results}
\end{figure*}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{images/TDrive_complete_varying_n.eps}
    \caption{$\delta = 8$, $\epsilon = 100$ and $\mu$ varying}
    \label{fig:tdrive_complete_vary_n}
\end{figure}

Our last dataset to evaluate is the Brinkhoff synthetic dataset. We were already able to achieve huge running time
improvements with BitDF, but we could achieve even more with our new multi-threaded model. Based on the dataset results
(\secref{sec:brinkhoff}), we chose the running parameters as follows: $\mu=4$, $\delta=4$ and $\epsilon=1200$.
\figref{fig:brinkhoff_threads} shows that we were always able to reduce the algorithm running time, even with the number
of worker threads being way higher than the number of available processing units in the machine. That is a result that
is completely different from those that we saw with the previous datasets. Compared with the single thread run, we could
reduce the running time by 96\%, being that highest cutback achieved with 30 worker threads (60 in total). We took a
closer look on that specific dataset running with a high number of threads, in order to try to find the reason that was
making it run better as the number of worker threads grow. As one can see in \figref{fig:brinkhoff_disks_threads}, the
number of disks that are being generated by the disk threads ($d_i$) are very close to the final number of disks that
are being inserted in the global disk set. That means that the each disk thread is able to eliminate the number a lot of
repeated and superset disks before returning them to be merged to the global set. Another thing that is important to
note is that the number of disks being generated is really small as time advances, meaning that the points are more
scattered over time, causing less disks to be generated and then less synchronization between shared queues for each
executing thread. Because of that, threads could spend less time blocked waiting for shared data to become available.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{images/Brinkhoff_thread.eps}
    \caption{Execution time reduction by number of threads for the Brinkhof dataset}
    \label{fig:brinkhoff_threads}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{images/Brinkhoff_disks_threads.eps}
    \caption{Sum of disks generated by the disk threads (red) and number of disks that were inserted in the global disk
        set after subset/superset check (blue), by time slot}
    \label{fig:brinkhoff_disks_threads}
\end{figure}

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/Brinkhoff_complete_varying_l.eps}
        \caption{$\mu = 4$, $\epsilon = 100$ and $\delta$ varying}
        \label{fig:brinkhoff_complete_vary_l}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/Brinkhoff_complete_varying_g.eps}
        \caption{$\mu = 4$, $\delta = 8$ and $\epsilon$ varying}
        \label{fig:brinkhoff_complete_vary_g}
    \end{subfigure}
    \caption{Results varying $\delta$ and $\epsilon$ for Brinkhoff dataset}
    \label{fig:brinkhoff_complete_results}
\end{figure*}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{images/Brinkhoff_complete_varying_n.eps}
    \caption{$\delta = 8$, $\epsilon = 100$ and $\mu$ varying}
    \label{fig:brinkhoff_complete_vary_n}
\end{figure}

In this section we could show that a somewhat simple remodeling in a system's architecture could lead to tremendous
running time improvements, by taking advantage of the multi-core paradigm. Our results show that we could reduce the
running time by as much as 96\%, when choosing the correct number of worker threads and the correct separation of work
that can be parallelized.
